import os
import json
import requests
from openai import OpenAI
from bs4 import BeautifulSoup
import pdfplumber
from typing import List, Dict
import time

# 1. ì„¤ì •
UPSTAGE_API_KEY = "api_key_here"  # Upstage API í‚¤ ì„¤ì •
target_file = "raw_data.pdf"
output_filename = "qa_data_full.json"
debug_folder = "debug_texts"

# ë””ë²„ê·¸ í´ë” ìƒì„±
os.makedirs(debug_folder, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì „ì²´ ì¼€ì´ìŠ¤ ì €ì¥
all_cases = []


def get_total_pages(pdf_path: str) -> int:
    """
    PDFì˜ ì „ì²´ í˜ì´ì§€ ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì „ì²´ í˜ì´ì§€ ìˆ˜
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            return len(pdf.pages)
    except Exception as e:
        print(f"í˜ì´ì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
        return 0


def extract_pages_with_overlap(pdf_path: str, start_page: int, end_page: int, 
                                overlap_pages: int = 1) -> str:
    """
    PDFì—ì„œ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë¥¼ ì¶”ì¶œí•˜ë˜, ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í¬í•¨í•˜ëŠ” í•¨ìˆ˜
    (ì§ˆë¬¸-ë‹µë³€ì´ í˜ì´ì§€ë¥¼ ê±¸ì³ìˆì„ ë•Œë¥¼ ëŒ€ë¹„)
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        start_page: ì‹œì‘ í˜ì´ì§€ (0ë¶€í„° ì‹œì‘)
        end_page: ë í˜ì´ì§€ (í¬í•¨)
        overlap_pages: ì´ì „ ì²­í¬ì™€ ê²¹ì¹˜ëŠ” í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 1í˜ì´ì§€)
    
    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    try:
        with pdfplumber.open(pdf_path) as pdf:
            # ì˜¤ë²„ë©ì„ ê³ ë ¤í•œ ì‹¤ì œ ì‹œì‘ í˜ì´ì§€
            # ì²« ë²ˆì§¸ ì²­í¬ê°€ ì•„ë‹ˆë©´ overlap_pagesë§Œí¼ ì•ì—ì„œ ì‹œì‘
            actual_start = max(0, start_page - overlap_pages) if start_page > 0 else start_page
            
            text = ""
            for page_num in range(actual_start, min(end_page + 1, len(pdf.pages))):
                page = pdf.pages[page_num]
                page_text = page.extract_text()
                
                if page_text:
                    # í˜ì´ì§€ êµ¬ë¶„ì ì¶”ê°€ (ë””ë²„ê¹…ìš©)
                    text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n"
                    text += page_text + "\n"
            
            return text
    except Exception as e:
        print(f"í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""


def parse_pdf_chunk_with_upstage(file_path: str, start_page: int, end_page: int) -> str:
    """
    Upstage APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì˜ íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜
    ì£¼ì˜: Upstage APIëŠ” ì „ì²´ íŒŒì¼ì„ ë°›ìœ¼ë¯€ë¡œ, ì´ í•¨ìˆ˜ëŠ” ì „ì²´ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ 
    í´ë¼ì´ì–¸íŠ¸ ì¸¡ì—ì„œ í˜ì´ì§€ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    
    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        start_page: ì‹œì‘ í˜ì´ì§€ ë²ˆí˜¸
        end_page: ë í˜ì´ì§€ ë²ˆí˜¸
    
    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    # ì‹¤ì œë¡œëŠ” pdfplumberë¡œ í˜ì´ì§€ë³„ ì¶”ì¶œì´ ë” íš¨ìœ¨ì 
    # Upstage APIëŠ” ì „ì²´ ë¬¸ì„œë¥¼ í•œë²ˆì— ì²˜ë¦¬í•˜ë¯€ë¡œ, ì²­í¬ë³„ë¡œëŠ” pdfplumber ì‚¬ìš©
    return extract_pages_with_overlap(file_path, start_page, end_page)


def extract_data_with_solar(text_content: str, chunk_num: int) -> str:
    """
    Solar Pro LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        text_content: ì¶”ì¶œí•  í…ìŠ¤íŠ¸ ë‚´ìš©
        chunk_num: í˜„ì¬ ì²­í¬ ë²ˆí˜¸ (ë””ë²„ê¹…ìš©)
    
    Returns:
        JSON í˜•ì‹ì˜ ë¬¸ìì—´
    """
    client = OpenAI(
        api_key=UPSTAGE_API_KEY,
        base_url="https://api.upstage.ai/v1"
    )

    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "construction_law_case",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "cases": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "case_id": {"type": "string"},
                                "date": {"type": "string"},
                                "category": {"type": "string"},
                                "title": {"type": "string"},
                                "content": {
                                    "type": "object",
                                    "properties": {
                                        "question": {"type": "string"},
                                        "answer": {"type": "string"},
                                        "reasoning": {"type": "string"}
                                    },
                                    "required": ["question", "answer", "reasoning"],
                                    "additionalProperties": False
                                },
                                "related_laws": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "code": {"type": "string"},
                                            "article_id": {"type": "string"}
                                        },
                                        "required": ["code", "article_id"],
                                        "additionalProperties": False
                                    }
                                }
                            },
                            "required": ["case_id", "title", "content", "related_laws"],
                            "additionalProperties": False
                        }
                    }
                },
                "required": ["cases"],
                "additionalProperties": False
            }
        }
    }

    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (Solar Proì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê³ ë ¤)
    input_text = text_content[:25000]
    
    messages = [
        {
            "role": "system",
            "content": """You are a legal data extraction assistant specialized in Korean construction law documents.

CRITICAL INSTRUCTIONS for data extraction:
1. Extract case_id, date, category, and title as metadata
2. For "question" field: Extract the ì§ˆì˜ (question) section
3. For "answer" field: Extract the íšŒì‹  (response/answer) section
4. For "reasoning" field: **COPY THE EXACT TEXT from the ì´ìœ (ì‚¬ìœ ) section WITHOUT ANY MODIFICATION, SUMMARIZATION, OR PARAPHRASING**
   - Include ALL the original text from the reasoning section
   - Preserve the exact wording, punctuation, and structure
   - Do NOT summarize or shorten the reasoning
   - Do NOT paraphrase or rewrite the reasoning
   - This is the most important field - accuracy is critical
5. Extract related_laws with proper law codes and article numbers

IMPORTANT: 
- If a case spans across page breaks (indicated by "--- í˜ì´ì§€ X ---"), treat it as a single continuous case
- Complete cases that start in this chunk but may be cut off at the end
- Skip incomplete cases that are clearly cut off at the beginning (these will be captured in the next chunk with overlap)

The reasoning section typically appears after the answer and explains the legal basis for the decision."""
        },
        {
            "role": "user",
            "content": f"""Extract all complete legal cases from the following Korean construction law document.

REMEMBER: 
1. For the "reasoning" field, you MUST copy the exact original text without any changes.
2. If a case is split across pages, combine them into one complete case.
3. Only extract complete cases - skip cases that are clearly cut off at the start.

Text to analyze (Chunk #{chunk_num}):

{input_text}"""
        }
    ]

    try:
        response = client.chat.completions.create(
            model="solar-pro2",
            messages=messages,
            response_format=response_format,
            temperature=0.1  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"LLM ì¶”ì¶œ ì˜¤ë¥˜ (ì²­í¬ {chunk_num}): {e}")
        return json.dumps({"cases": []})


def deduplicate_cases(cases: List[Dict]) -> List[Dict]:
    """
    ì¤‘ë³µëœ ì¼€ì´ìŠ¤ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    case_idì™€ titleì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ íŒë‹¨
    
    Args:
        cases: ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    seen = set()
    unique_cases = []
    
    for case in cases:
        # case_idê°€ ìˆìœ¼ë©´ case_idë¡œ, ì—†ìœ¼ë©´ titleë¡œ ì¤‘ë³µ ì²´í¬
        identifier = case.get('case_id', '') or case.get('title', '')
        
        if identifier and identifier not in seen:
            seen.add(identifier)
            unique_cases.append(case)
        elif not identifier:
            # identifierê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨ (ì¶”í›„ ìˆ˜ë™ í™•ì¸ í•„ìš”)
            unique_cases.append(case)
    
    return unique_cases


def process_pdf_in_chunks(pdf_path: str, chunk_size: int = 5, overlap: int = 1):
    """
    PDFë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        chunk_size: í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 5í˜ì´ì§€)
        overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 1í˜ì´ì§€)
    """
    print("=" * 80)
    print(f"PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
    print("=" * 80)
    
    # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    total_pages = get_total_pages(pdf_path)
    if total_pages == 0:
        print("ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì „ì²´ í˜ì´ì§€ ìˆ˜: {total_pages}")
    print(f"ì²­í¬ í¬ê¸°: {chunk_size}í˜ì´ì§€")
    print(f"ì˜¤ë²„ë©: {overlap}í˜ì´ì§€")
    print(f"ì˜ˆìƒ ì²­í¬ ìˆ˜: {(total_pages + chunk_size - 1) // chunk_size}")
    print("=" * 80)
    
    # ì²­í¬ë³„ë¡œ ì²˜ë¦¬
    chunk_num = 0
    for start_page in range(0, total_pages, chunk_size):
        chunk_num += 1
        end_page = min(start_page + chunk_size - 1, total_pages - 1)
        
        print(f"\n{'='*80}")
        print(f"ì²­í¬ {chunk_num} ì²˜ë¦¬ ì¤‘: í˜ì´ì§€ {start_page + 1} ~ {end_page + 1}")
        print(f"{'='*80}")
        
        # 1. í˜ì´ì§€ ì¶”ì¶œ (ì˜¤ë²„ë© í¬í•¨)
        print(f"  [1/4] í˜ì´ì§€ ì¶”ì¶œ ì¤‘...")
        text_content = extract_pages_with_overlap(pdf_path, start_page, end_page, overlap)
        
        if not text_content or len(text_content) < 100:
            print(f"  âš ï¸  ê²½ê³ : ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({len(text_content)}ì). ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue
        
        print(f"  âœ… ì¶”ì¶œ ì™„ë£Œ: {len(text_content):,}ì")
        
        # ë””ë²„ê·¸: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥
        debug_file = os.path.join(debug_folder, f"chunk_{chunk_num:03d}_pages_{start_page+1}-{end_page+1}.txt")
        with open(debug_file, "w", encoding="utf-8") as f:
            f.write(text_content)
        print(f"  ğŸ“ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥: {debug_file}")
        
        # 2. LLMìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ
        print(f"  [2/4] LLM ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        json_result = extract_data_with_solar(text_content, chunk_num)
        
        # 3. JSON íŒŒì‹±
        print(f"  [3/4] JSON íŒŒì‹± ì¤‘...")
        try:
            chunk_data = json.loads(json_result)
            chunk_cases = chunk_data.get("cases", [])
            print(f"  âœ… ì¶”ì¶œëœ ì¼€ì´ìŠ¤ ìˆ˜: {len(chunk_cases)}ê°œ")
            
            # 4. ì „ì²´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            all_cases.extend(chunk_cases)
            
        except json.JSONDecodeError as e:
            print(f"  âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
        
        # API ì†ë„ ì œí•œì„ ìœ„í•œ ëŒ€ê¸° (í•„ìš”ì‹œ)
        if chunk_num < (total_pages + chunk_size - 1) // chunk_size:
            print(f"  [4/4] ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•´ 2ì´ˆ ëŒ€ê¸°...")
            time.sleep(2)
    
    print(f"\n{'='*80}")
    print(f"ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì´ ì¶”ì¶œëœ ì¼€ì´ìŠ¤ ìˆ˜ (ì¤‘ë³µ í¬í•¨): {len(all_cases)}ê°œ")
    
    # ì¤‘ë³µ ì œê±°
    print(f"\nì¤‘ë³µ ì¼€ì´ìŠ¤ ì œê±° ì¤‘...")
    unique_cases = deduplicate_cases(all_cases)
    print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_cases)}ê°œ -> {len(unique_cases)}ê°œ")
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    print(f"\nìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘: {output_filename}")
    final_data = {"cases": unique_cases}
    
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ!")
    
    # í†µê³„ ì¶œë ¥
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ìµœì¢… í†µê³„")
    print(f"{'='*80}")
    print(f"  ì „ì²´ í˜ì´ì§€: {total_pages}í˜ì´ì§€")
    print(f"  ì²˜ë¦¬ëœ ì²­í¬: {chunk_num}ê°œ")
    print(f"  ì¶”ì¶œëœ ì¼€ì´ìŠ¤: {len(unique_cases)}ê°œ")
    print(f"  ì œê±°ëœ ì¤‘ë³µ: {len(all_cases) - len(unique_cases)}ê°œ")
    
    # ìƒ˜í”Œ ì¼€ì´ìŠ¤ ì¶œë ¥
    if len(unique_cases) > 0:
        print(f"\n{'='*80}")
        print(f"ğŸ“„ ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°")
        print(f"{'='*80}")
        first_case = unique_cases[0]
        print(f"  Case ID: {first_case.get('case_id', 'N/A')}")
        print(f"  Date: {first_case.get('date', 'N/A')}")
        print(f"  Category: {first_case.get('category', 'N/A')}")
        print(f"  Title: {first_case.get('title', 'N/A')[:80]}...")
        print(f"  Question ê¸¸ì´: {len(first_case['content']['question'])}ì")
        print(f"  Answer ê¸¸ì´: {len(first_case['content']['answer'])}ì")
        print(f"  Reasoning ê¸¸ì´: {len(first_case['content']['reasoning'])}ì")
        print(f"  Related Laws: {len(first_case.get('related_laws', []))}ê°œ")


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    try:
        process_pdf_in_chunks(
            pdf_path=target_file,
            chunk_size=3,  # 3í˜ì´ì§€ì”© ì²˜ë¦¬
            overlap=1      # 1í˜ì´ì§€ ì˜¤ë²„ë©
        )
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()